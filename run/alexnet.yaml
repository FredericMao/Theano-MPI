# GoogLeNet model and training hyperparameters

# Learning Parameters
n_epochs: 70 # def: 70 for  AlexNet, 90 for GoogLeNet
learning_rate: 0.01  # def: 0.01 for both AlexNet (256b) and GoogLeNet (32b), scale down when batchsize lowered
lr_policy: step  # def: step or auto
lr_adapt_threshold: 0.01
lr_step: [20, 40, 60]   # def: 20, learning rate will be reduced by 10 times at these steps when training AlexNet

# EASGD parameters
# server_alpha: 0.9
# worker_alpha: 0.5
# alpha_step: [10, 30, 40] # asymmetric alpha, every step server_alpha will minus 0.3
# alpha_minus: 0.2

server_alpha: 0.5
worker_alpha: 0.5
alpha_step: [10, 30, 50] 
alpha_minus: 0 # asymmetric alpha, every step server_alpha will minus this

# Weight Decay
weight_decay: 0.0005 # def: 0.0005 for AlexNet, 0.0002 for GoogLeNet

# Momentum
momentum: 0.9 # def: 0.9

# Model info
name: alexnet
input_width: 227 # def: 227 for AlexNet, 224 for GoogLeNet 
input_height: 227 # def: 227 for AlexNet, 224 for GoogLeNet 

n_softmax_out: 1000

batch_size: 128 # def: 128 # def: 128 for AlexNet, 32 for GoogleNet 

image_mean: img_mean

data: imagenet
# Directories
dir_head : /scratch/ilsvrc12/#/work/mahe6562/prepdata_1000cat_128b/   # base dir where hkl training data is kept
label_folder : /labels/  # 
mean_file : /misc/img_mean.npy   
train_folder: /train_hkl_b256_b_128/   #/hkl_data/  
val_folder: /val_hkl_b256_b_128/   
# Data
file_batch_size: 128 # def: choose according to the preprocessed hkl file size
