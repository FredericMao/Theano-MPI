# This version of config.yaml is modified based on the theano_alexnet project. See the original project here:
# https://github.com/uoguelph-mlrg/theano_alexnet, and its copy right:
# Copyright (c) 2014, Weiguang Ding, Ruoyan Wang, Fei Mao and Graham Taylor
# All rights reserved.

# If want to input None, use !!null

# Model choose
name: cnncifar10 # alexnet, googlenet, vggnet or cnncifar10

monitor_grad: False
verbose: True

pretrain: False

# Resume Training, start from scratch or resume training
resume_train: False # load the following parameters during training. Val will load the parameters regardless of this
load_epoch: 20
load_path: ./pretrained-models/ # edit here to find your pretrained weights
weights_dir : ./models # directory for saving weights and results
record_dir: ./inforec/
initial_val: False


# Parameter Exchanging
sync_start: True # start worker process together with server process in one mpirun call
sync_freq: 1 # def: 1, if average weights every iteration in EASGD

sync_type: cdd # mode selection: avg or cdd, EASGD or ASGD

exch_strategy: nccl32 # asa32, asa16, copper, copper16, nccl16, nccl32 or ar

para_load: False # def: should be always true, training and loading data in parallel

# conv library
lib_conv: cudnn

# output
flag_top_5: True
print_info_every: 20480 # print time and error info every 5120 images
snapshot_freq: 2  # epoch frequency of saving weights def: 20
print_train_error: True
print_freq: 2  # iteration frequency of printing training error rate def: 200

# randomness
shuffle: True # def: True, if shuffle the batches
rand_crop: True # def: True, if False will likely be overfitting
batch_crop_mirror: False  # if False, do randomly on each image separately
weight_init_seed: 23455 # to change, see layer.py file
dropout_init_seed: 0 # to change, see layer.py file

# gpu and socket
sock_data: 5020
debug: False # def: False ; True: run a small sample of training data for shorter epoch testing time
